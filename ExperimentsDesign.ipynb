{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1171252-1589-4532-9e9a-62ef35bb1669",
   "metadata": {},
   "source": [
    "Примечание: часть функционала все еще находится в процессе разработки и тестирования, поэтому может содержать ошибки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f7343e-68dd-45e4-b3f7-dd953a31d4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.17). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim.lr_scheduler\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import time\n",
    "from lion_pytorch import Lion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc3963f-d127-4683-b6ee-124bf57b93b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Классы датасетов (старые, больше не используются из-за более низкой производительности, но при том же количестве эпох дает результат не хуже нового, но это занимает значительно больше времени, и возникает высокая зависимость от скорости диска)\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, folder) -> None:\n",
    "        images_folder = folder\n",
    "        gt_folder = 'seg-lungs-LUNA16'\n",
    "        self.images = []\n",
    "        self.masks = []\n",
    "\n",
    "        for fname in os.listdir(images_folder):\n",
    "            if '.mhd' in fname:\n",
    "                img_path = os.path.join(images_folder, fname)\n",
    "                mask_path = os.path.join(gt_folder, fname)\n",
    "                self.images.append(img_path)\n",
    "                self.masks.append(mask_path)\n",
    "\n",
    "        self.transform = A.Compose([\n",
    "            A.Resize(128, 128),\n",
    "            #A.RandomResizedCrop(height=128, width=128, scale=(0.95, 1.0), ratio=(1.0, 1.0), p=1),\n",
    "            #A.Rotate(limit=5, p=0.7),\n",
    "            \n",
    "            #A.RandomBrightnessContrast(brightness_limit=0.05, contrast_limit=0.05, p=0.5),\n",
    "            #A.GaussNoise(var_limit=(5.0, 20.0), p=0.1),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = sitk.ReadImage(self.images[index])\n",
    "        slice_num = random.randint(0, img.GetDepth()-1)\n",
    "        img = sitk.GetArrayFromImage(img[:,:,slice_num])\n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(self.masks[index])[:,:,slice_num])\n",
    "        img = img.astype(np.float32)\n",
    "        img = ((img - img.min()) / (img.max() - img.min()))\n",
    "\n",
    "        transformed_all = self.transform(image=img, mask=mask)\n",
    "        transformed_img = transformed_all['image']\n",
    "        transformed_mask = transformed_all['mask']\n",
    "        return transformed_img, transformed_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "class ValDataset(Dataset):\n",
    "    def __init__(self, folder) -> None:\n",
    "        images_folder = folder\n",
    "        gt_folder = 'seg-lungs-LUNA16'\n",
    "        self.images = []\n",
    "        self.masks = []\n",
    "\n",
    "        for fname in os.listdir(images_folder):\n",
    "            if '.mhd' in fname:\n",
    "                img_path = os.path.join(images_folder, fname)\n",
    "                mask_path = os.path.join(gt_folder, fname)\n",
    "                self.images.append(img_path)\n",
    "                self.masks.append(mask_path)\n",
    "\n",
    "        self.transform = A.Compose([\n",
    "            A.Resize(128, 128),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = sitk.ReadImage(self.images[index])\n",
    "        slice_num = random.randint(0, img.GetDepth()-1)\n",
    "        img = sitk.GetArrayFromImage(img[:,:,slice_num])\n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(self.masks[index])[:,:,slice_num])\n",
    "        img = img.astype(np.float32)\n",
    "        img = ((img - img.min()) / (img.max() - img.min()))\n",
    "\n",
    "        transformed_all = self.transform(image=img, mask=mask)\n",
    "        transformed_img = transformed_all['image']\n",
    "        transformed_mask = transformed_all['mask']\n",
    "        return transformed_img, transformed_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5389b15d-8f53-4765-8dbb-0ab207d91f77",
   "metadata": {},
   "source": [
    "Было проведено еще несколько попыток изменить класс датасета (их не привожу из-за большого объема), которые в итоге привели к оптимальному и гибкому варианту:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ba50fcf-c73e-44fb-a1c6-bd6e80c727b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Класс датасета (оптимальный для работы с разным объемом оперативной памяти, используется в проекте)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, folder, gt_folder, size, batch, max_uses_per_scan, transforms) -> None:\n",
    "        self.images = []\n",
    "        self.masks = []\n",
    "        self.batch = batch #количество исследований КТ, загружаемых одновременно в оперативную память\n",
    "        self.max_uses_per_scan = max_uses_per_scan #сколько в среднем раз будет браться случайный срез из каждого КТ до перезагрузки в оперативную память\n",
    "        self.folder = folder \n",
    "        self.gt_folder = gt_folder\n",
    "        self.counter = 0\n",
    "        self.size = size\n",
    "\n",
    "        self.mapping = {0: 0, 3: 1, 4: 2, 5: 3}\n",
    "        self.lookup_table = self._create_lookup_table()\n",
    "        \n",
    "        for fname in os.listdir(folder):\n",
    "            if '.mhd' in fname:\n",
    "                img_path = os.path.join(folder, fname)\n",
    "                mask_path = os.path.join(gt_folder, fname)\n",
    "                self.images.append(img_path)\n",
    "                self.masks.append(mask_path)\n",
    "\n",
    "        self.transform = transforms\n",
    "        self.current_ct_index = 0\n",
    "        self.scans = list(zip(self.images, self.masks))\n",
    "        random.shuffle(self.scans)\n",
    "        self.load()\n",
    "\n",
    "    def _create_lookup_table(self):\n",
    "        max_label = max(self.mapping.keys())\n",
    "        lookup = np.zeros(max_label + 1, dtype=np.int64)\n",
    "        for original_label, new_label in self.mapping.items():\n",
    "            lookup[original_label] = new_label\n",
    "        return lookup\n",
    "        \n",
    "    def load(self):\n",
    "        if self.current_ct_index + self.batch > len(self.images):\n",
    "            self.current_ct_index = 0\n",
    "            random.shuffle(self.scans)\n",
    "\n",
    "        img_filenames = self.scans[self.current_ct_index : self.current_ct_index + self.batch] #pairs\n",
    "\n",
    "        img_pathes = [img_filename[0] for img_filename in img_filenames]\n",
    "        mask_pathes = [mask_filename[1] for mask_filename in img_filenames]\n",
    "\n",
    "        self.imgs = [sitk.ReadImage(img_path) for img_path in img_pathes]\n",
    "        self.masks = [sitk.ReadImage(mask_path) for mask_path in mask_pathes]\n",
    "        \n",
    "        self.current_ct_index += self.batch\n",
    "        self.counter = 0\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.counter >= self.max_uses_per_scan * self.batch:\n",
    "            self.load()\n",
    "\n",
    "        index = random.randint(0, self.batch - 1)\n",
    "        img = self.imgs[index]\n",
    "        \n",
    "        slice_num = random.randint(0, img.GetDepth()-1)\n",
    "        \n",
    "        img = sitk.GetArrayFromImage(img[:,:,slice_num])\n",
    "        mask = sitk.GetArrayFromImage(self.masks[index][:,:,slice_num])\n",
    "        \n",
    "        img = img.astype(np.float32)\n",
    "        img = ((img - img.min()) / (img.max() - img.min()))\n",
    "\n",
    "        mask = self.lookup_table[mask]\n",
    "\n",
    "        self.counter += 1\n",
    "\n",
    "        transformed_all = self.transform(image=img, mask=mask)\n",
    "        transformed_img = transformed_all['image']\n",
    "        transformed_mask = transformed_all['mask']\n",
    "        return transformed_img, transformed_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c247fbe4-c8d6-42d1-8623-95762c2f0a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Небольшая нейросеть U-Net для тестирования работоспособности и использования в качестве бейзлайна\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        skip_connection = x #перед relu, чтобы сохранить детали\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x, skip_connection\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, mid_channels, kernel_size=2, stride=2)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x, skip_connection):\n",
    "        x = self.upconv(x)\n",
    "        x = torch.cat([x, skip_connection], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class MiniUnet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder1 = EncoderBlock(in_channels, 64)\n",
    "        self.encoder2 = EncoderBlock(64, 128)\n",
    "\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.decoder1 = DecoderBlock(256, 128, 128)\n",
    "        self.decoder2 = DecoderBlock(128, 64, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, skip1 = self.encoder1(x)\n",
    "        x, skip2 = self.encoder2(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        x = self.decoder1(x, skip2)\n",
    "        x = self.decoder2(x, skip1)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8039db-8101-4b4b-a7a9-ca0339209999",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Классический U-Net\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        skip_connection = x #перед relu, чтобы сохранить детали\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x, skip_connection\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, mid_channels, kernel_size=2, stride=2)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x, skip_connection):\n",
    "        x = self.upconv(x)\n",
    "        x = torch.cat([x, skip_connection], dim=1) #стакнули изображения\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder1 = EncoderBlock(in_channels, 64)\n",
    "        self.encoder2 = EncoderBlock(64, 128)\n",
    "        self.encoder3 = EncoderBlock(128, 256)\n",
    "        self.encoder4 = EncoderBlock(256, 512)\n",
    "\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.decoder1 = DecoderBlock(1024, 512, 512)\n",
    "        self.decoder2 = DecoderBlock(512, 256, 256)\n",
    "        self.decoder3 = DecoderBlock(256, 128, 128)\n",
    "        self.decoder4 = DecoderBlock(128, 64, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, skip1 = self.encoder1(x)\n",
    "        x, skip2 = self.encoder2(x)\n",
    "        x, skip3 = self.encoder3(x)\n",
    "        x, skip4 = self.encoder4(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        x = self.decoder1(x, skip4)\n",
    "        x = self.decoder2(x, skip3)\n",
    "        x = self.decoder3(x, skip2)\n",
    "        x = self.decoder4(x, skip1)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51368286-f87b-42d2-b160-4b53cdfb786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#увеличенный U-Net (требует слишком много вычислительных ресурсов)\n",
    "class MaxUnet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder1 = EncoderBlock(in_channels, 64)\n",
    "        self.encoder2 = EncoderBlock(64, 128)\n",
    "        self.encoder3 = EncoderBlock(128, 256)\n",
    "        self.encoder4 = EncoderBlock(256, 512)\n",
    "        self.encoder5 = EncoderBlock(512, 1024)\n",
    "\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(1024, 2048, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(2048, 2048, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(2048, 2048, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.ReLU(inplace=True),            \n",
    "        )\n",
    "\n",
    "        self.decoder1 = DecoderBlock(2048, 1024, 1024)\n",
    "        self.decoder2 = DecoderBlock(1024, 512, 512)\n",
    "        self.decoder3 = DecoderBlock(512, 256, 256)\n",
    "        self.decoder4 = DecoderBlock(256, 128, 128)\n",
    "        self.decoder5 = DecoderBlock(128, 64, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, skip1 = self.encoder1(x)\n",
    "        x, skip2 = self.encoder2(x)\n",
    "        x, skip3 = self.encoder3(x)\n",
    "        x, skip4 = self.encoder4(x)\n",
    "        x, skip5 = self.encoder5(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        x = self.decoder1(x, skip5)\n",
    "        x = self.decoder2(x, skip4)\n",
    "        x = self.decoder3(x, skip3)\n",
    "        x = self.decoder4(x, skip2)\n",
    "        x = self.decoder5(x, skip1)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eb1740-1811-439b-8ad4-0889f921ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Архитектура U-Net++\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class NestedDecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        inputs = [x for x in inputs if x is not None]\n",
    "        x = torch.cat(inputs, dim=1)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class UnetPlusPlus(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=4):\n",
    "        super().__init__()\n",
    "\n",
    "        nb_filter = [32, 64, 128, 256, 512]\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv0_0 = EncoderBlock(in_channels, nb_filter[0])\n",
    "        self.conv1_0 = EncoderBlock(nb_filter[0], nb_filter[1])\n",
    "        self.conv2_0 = EncoderBlock(nb_filter[1], nb_filter[2])\n",
    "        self.conv3_0 = EncoderBlock(nb_filter[2], nb_filter[3])\n",
    "        self.conv4_0 = EncoderBlock(nb_filter[3], nb_filter[4])\n",
    "\n",
    "        self.conv0_1 = NestedDecoderBlock(nb_filter[0]+nb_filter[1], nb_filter[0])\n",
    "        self.conv1_1 = NestedDecoderBlock(nb_filter[1]+nb_filter[2], nb_filter[1])\n",
    "        self.conv2_1 = NestedDecoderBlock(nb_filter[2]+nb_filter[3], nb_filter[2])\n",
    "        self.conv3_1 = NestedDecoderBlock(nb_filter[3]+nb_filter[4], nb_filter[3])\n",
    "\n",
    "        self.conv0_2 = NestedDecoderBlock(nb_filter[0]*2+nb_filter[1], nb_filter[0])\n",
    "        self.conv1_2 = NestedDecoderBlock(nb_filter[1]*2+nb_filter[2], nb_filter[1])\n",
    "        self.conv2_2 = NestedDecoderBlock(nb_filter[2]*2+nb_filter[3], nb_filter[2])\n",
    "\n",
    "        self.conv0_3 = NestedDecoderBlock(nb_filter[0]*3+nb_filter[1], nb_filter[0])\n",
    "        self.conv1_3 = NestedDecoderBlock(nb_filter[1]*3+nb_filter[2], nb_filter[1])\n",
    "\n",
    "        self.conv0_4 = NestedDecoderBlock(nb_filter[0]*4+nb_filter[1], nb_filter[0])\n",
    "\n",
    "        self.final = nn.Conv2d(nb_filter[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x0_0 = self.conv0_0(input)\n",
    "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
    "        x0_1 = self.conv0_1(torch.cat([x0_0, self.up(x1_0)], 1))\n",
    "\n",
    "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
    "        x1_1 = self.conv1_1(torch.cat([x1_0, self.up(x2_0)], 1))\n",
    "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.up(x1_1)], 1))\n",
    "\n",
    "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
    "        x2_1 = self.conv2_1(torch.cat([x2_0, self.up(x3_0)], 1))\n",
    "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up(x2_1)], 1))\n",
    "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.up(x1_2)], 1))\n",
    "\n",
    "        x4_0 = self.conv4_0(self.pool(x3_0))\n",
    "        x3_1 = self.conv3_1(torch.cat([x3_0, self.up(x4_0)], 1))\n",
    "        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.up(x3_1)], 1))\n",
    "        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.up(x2_2)], 1))\n",
    "        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.up(x1_3)], 1))\n",
    "\n",
    "        output = self.final(x0_4)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ae5b5b-889e-46a8-be55-c939b6862fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Первая версия U-Net++ с механизмом внимания (механизм внимания применяется только в конце)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class NestedDecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        inputs = [x for x in inputs if x is not None]\n",
    "        x = torch.cat(inputs, dim=1)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class AttentionGate(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(AttentionGate, self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "class AttentionUnetPlusPlus1(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=4):\n",
    "        super().__init__()\n",
    "\n",
    "        nb_filter = [32, 64, 128, 256, 512]\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv0_0 = EncoderBlock(in_channels, nb_filter[0])\n",
    "        self.conv1_0 = EncoderBlock(nb_filter[0], nb_filter[1])\n",
    "        self.conv2_0 = EncoderBlock(nb_filter[1], nb_filter[2])\n",
    "        self.conv3_0 = EncoderBlock(nb_filter[2], nb_filter[3])\n",
    "        self.conv4_0 = EncoderBlock(nb_filter[3], nb_filter[4])\n",
    "\n",
    "        self.conv0_1 = NestedDecoderBlock(nb_filter[0]+nb_filter[1], nb_filter[0])\n",
    "        self.conv1_1 = NestedDecoderBlock(nb_filter[1]+nb_filter[2], nb_filter[1])\n",
    "        self.conv2_1 = NestedDecoderBlock(nb_filter[2]+nb_filter[3], nb_filter[2])\n",
    "        self.conv3_1 = NestedDecoderBlock(nb_filter[3]+nb_filter[4], nb_filter[3])\n",
    "\n",
    "        self.conv0_2 = NestedDecoderBlock(nb_filter[0]*2+nb_filter[1], nb_filter[0])\n",
    "        self.conv1_2 = NestedDecoderBlock(nb_filter[1]*2+nb_filter[2], nb_filter[1])\n",
    "        self.conv2_2 = NestedDecoderBlock(nb_filter[2]*2+nb_filter[3], nb_filter[2])\n",
    "\n",
    "        self.conv0_3 = NestedDecoderBlock(nb_filter[0]*3+nb_filter[1], nb_filter[0])\n",
    "        self.conv1_3 = NestedDecoderBlock(nb_filter[1]*3+nb_filter[2], nb_filter[1])\n",
    "\n",
    "        self.conv0_4 = NestedDecoderBlock(nb_filter[0]*4+nb_filter[1], nb_filter[0])\n",
    "\n",
    "        #Attention Gates\n",
    "        self.attention1 = AttentionGate(F_g=nb_filter[1], F_l=nb_filter[0], F_int=nb_filter[0]//2)\n",
    "        self.attention2 = AttentionGate(F_g=nb_filter[2], F_l=nb_filter[1], F_int=nb_filter[1]//2)\n",
    "        self.attention3 = AttentionGate(F_g=nb_filter[3], F_l=nb_filter[2], F_int=nb_filter[2]//2)\n",
    "        self.attention4 = AttentionGate(F_g=nb_filter[4], F_l=nb_filter[3], F_int=nb_filter[3]//2)\n",
    "\n",
    "        self.final = nn.Conv2d(nb_filter[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x0_0 = self.conv0_0(input)\n",
    "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
    "        x0_1 = self.conv0_1(torch.cat([x0_0, self.up(x1_0)], 1))\n",
    "\n",
    "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
    "        x1_1 = self.conv1_1(torch.cat([x1_0, self.up(x2_0)], 1))\n",
    "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.up(x1_1)], 1))\n",
    "\n",
    "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
    "        x2_1 = self.conv2_1(torch.cat([x2_0, self.up(x3_0)], 1))\n",
    "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up(x2_1)], 1))\n",
    "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.up(x1_2)], 1))\n",
    "\n",
    "        x4_0 = self.conv4_0(self.pool(x3_0))\n",
    "        x3_1 = self.conv3_1(torch.cat([x3_0, self.up(x4_0)], 1))\n",
    "        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.up(x3_1)], 1))\n",
    "        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.up(x2_2)], 1))\n",
    "        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.up(x1_3)], 1))\n",
    "\n",
    "        x0_1 = self.attention1(g=self.up(x1_0), x=x0_1)\n",
    "        x1_1 = self.attention2(g=self.up(x2_0), x=x1_1)\n",
    "        x2_1 = self.attention3(g=self.up(x3_0), x=x2_1)\n",
    "        x3_1 = self.attention4(g=self.up(x4_0), x=x3_1)\n",
    "\n",
    "        output = self.final(x0_4)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a74101-3523-44e6-9087-5204cb349a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Вторая версия U-Net++ с механизмом внимания (внедрение механизма внимания в skip-connections)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "class NestedDecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        inputs = [x for x in inputs if x is not None]\n",
    "        x = torch.cat(inputs, dim=1)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class AttentionUnetPlusPlus2(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=8):\n",
    "        super().__init__()\n",
    "\n",
    "        nb_filter = [32, 64, 128, 256, 512]\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv0_0 = EncoderBlock(in_channels, nb_filter[0])\n",
    "        self.conv1_0 = EncoderBlock(nb_filter[0], nb_filter[1])\n",
    "        self.conv2_0 = EncoderBlock(nb_filter[1], nb_filter[2])\n",
    "        self.conv3_0 = EncoderBlock(nb_filter[2], nb_filter[3])\n",
    "        self.conv4_0 = EncoderBlock(nb_filter[3], nb_filter[4])\n",
    "\n",
    "        self.attention1 = AttentionBlock(F_g=nb_filter[1], F_l=nb_filter[0], F_int=nb_filter[0]//2)\n",
    "        self.attention2 = AttentionBlock(F_g=nb_filter[2], F_l=nb_filter[1], F_int=nb_filter[1]//2)\n",
    "        self.attention3 = AttentionBlock(F_g=nb_filter[3], F_l=nb_filter[2], F_int=nb_filter[2]//2)\n",
    "        self.attention4 = AttentionBlock(F_g=nb_filter[4], F_l=nb_filter[3], F_int=nb_filter[3]//2)\n",
    "\n",
    "        self.conv0_1 = NestedDecoderBlock(nb_filter[0]+nb_filter[1], nb_filter[0])\n",
    "        self.conv1_1 = NestedDecoderBlock(nb_filter[1]+nb_filter[2], nb_filter[1])\n",
    "        self.conv2_1 = NestedDecoderBlock(nb_filter[2]+nb_filter[3], nb_filter[2])\n",
    "        self.conv3_1 = NestedDecoderBlock(nb_filter[3]+nb_filter[4], nb_filter[3])\n",
    "\n",
    "        self.conv0_2 = NestedDecoderBlock(nb_filter[0]*2+nb_filter[1], nb_filter[0])\n",
    "        self.conv1_2 = NestedDecoderBlock(nb_filter[1]*2+nb_filter[2], nb_filter[1])\n",
    "        self.conv2_2 = NestedDecoderBlock(nb_filter[2]*2+nb_filter[3], nb_filter[2])\n",
    "\n",
    "        self.conv0_3 = NestedDecoderBlock(nb_filter[0]*3+nb_filter[1], nb_filter[0])\n",
    "        self.conv1_3 = NestedDecoderBlock(nb_filter[1]*3+nb_filter[2], nb_filter[1])\n",
    "\n",
    "        self.conv0_4 = NestedDecoderBlock(nb_filter[0]*4+nb_filter[1], nb_filter[0])\n",
    "\n",
    "        self.final = nn.Conv2d(nb_filter[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x0_0 = self.conv0_0(input)\n",
    "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
    "\n",
    "        # Attention and Up-sampling\n",
    "        x0_0_att = self.attention1(g=self.up(x1_0), x=x0_0)\n",
    "        x0_1 = self.conv0_1(torch.cat([x0_0_att, self.up(x1_0)], 1))\n",
    "\n",
    "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
    "        x1_0_att = self.attention2(g=self.up(x2_0), x=x1_0)\n",
    "        x1_1 = self.conv1_1(torch.cat([x1_0_att, self.up(x2_0)], 1))\n",
    "\n",
    "        x0_1_att = self.attention1(g=self.up(x1_1), x=x0_1)\n",
    "        x0_2 = self.conv0_2(torch.cat([x0_0_att, x0_1_att, self.up(x1_1)], 1))\n",
    "\n",
    "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
    "        x2_0_att = self.attention3(g=self.up(x3_0), x=x2_0)\n",
    "        x2_1 = self.conv2_1(torch.cat([x2_0_att, self.up(x3_0)], 1))\n",
    "\n",
    "        x1_1_att = self.attention2(g=self.up(x2_1), x=x1_1)\n",
    "        x1_2 = self.conv1_2(torch.cat([x1_0_att, x1_1_att, self.up(x2_1)], 1))\n",
    "\n",
    "        x0_2_att = self.attention1(g=self.up(x1_2), x=x0_2)\n",
    "        x0_3 = self.conv0_3(torch.cat([x0_0_att, x0_1_att, x0_2_att, self.up(x1_2)], 1))\n",
    "\n",
    "        x4_0 = self.conv4_0(self.pool(x3_0))\n",
    "        x3_0_att = self.attention4(g=self.up(x4_0), x=x3_0)\n",
    "        x3_1 = self.conv3_1(torch.cat([x3_0_att, self.up(x4_0)], 1))\n",
    "\n",
    "        x2_1_att = self.attention3(g=self.up(x3_1), x=x2_1)\n",
    "        x2_2 = self.conv2_2(torch.cat([x2_0_att, x2_1_att, self.up(x3_1)], 1))\n",
    "\n",
    "        x1_2_att = self.attention2(g=self.up(x2_2), x=x1_2)\n",
    "        x1_3 = self.conv1_3(torch.cat([x1_0_att, x1_1_att, x1_2_att, self.up(x2_2)], 1))\n",
    "\n",
    "        x0_3_att = self.attention1(g=self.up(x1_3), x=x0_3)\n",
    "        x0_4 = self.conv0_4(torch.cat([x0_0_att, x0_1_att, x0_2_att, x0_3_att, self.up(x1_3)], 1))\n",
    "\n",
    "        output = self.final(x0_4)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c984a0-9c98-4fb3-bb3b-04f85697d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Третья версия U-Net++ с механизмом внимания (введение residual connections)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        out = self.conv(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "class NestedDecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        inputs = [x for x in inputs if x is not None]\n",
    "        x = torch.cat(inputs, dim=1)\n",
    "        residual = self.shortcut(x)\n",
    "        out = self.conv(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class AttentionUnetPlusPlus3(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=8):\n",
    "        super().__init__()\n",
    "\n",
    "        nb_filter = [32, 64, 128, 256, 512]\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv0_0 = EncoderBlock(in_channels, nb_filter[0])\n",
    "        self.conv1_0 = EncoderBlock(nb_filter[0], nb_filter[1])\n",
    "        self.conv2_0 = EncoderBlock(nb_filter[1], nb_filter[2])\n",
    "        self.conv3_0 = EncoderBlock(nb_filter[2], nb_filter[3])\n",
    "        self.conv4_0 = EncoderBlock(nb_filter[3], nb_filter[4])\n",
    "\n",
    "        self.attention1 = AttentionBlock(F_g=nb_filter[1], F_l=nb_filter[0], F_int=nb_filter[0]//2)\n",
    "        self.attention2 = AttentionBlock(F_g=nb_filter[2], F_l=nb_filter[1], F_int=nb_filter[1]//2)\n",
    "        self.attention3 = AttentionBlock(F_g=nb_filter[3], F_l=nb_filter[2], F_int=nb_filter[2]//2)\n",
    "        self.attention4 = AttentionBlock(F_g=nb_filter[4], F_l=nb_filter[3], F_int=nb_filter[3]//2)\n",
    "\n",
    "        self.conv0_1 = NestedDecoderBlock(nb_filter[0]+nb_filter[1], nb_filter[0])\n",
    "        self.conv1_1 = NestedDecoderBlock(nb_filter[1]+nb_filter[2], nb_filter[1])\n",
    "        self.conv2_1 = NestedDecoderBlock(nb_filter[2]+nb_filter[3], nb_filter[2])\n",
    "        self.conv3_1 = NestedDecoderBlock(nb_filter[3]+nb_filter[4], nb_filter[3])\n",
    "\n",
    "        self.conv0_2 = NestedDecoderBlock(nb_filter[0]*2+nb_filter[1], nb_filter[0])\n",
    "        self.conv1_2 = NestedDecoderBlock(nb_filter[1]*2+nb_filter[2], nb_filter[1])\n",
    "        self.conv2_2 = NestedDecoderBlock(nb_filter[2]*2+nb_filter[3], nb_filter[2])\n",
    "\n",
    "        self.conv0_3 = NestedDecoderBlock(nb_filter[0]*3+nb_filter[1], nb_filter[0])\n",
    "        self.conv1_3 = NestedDecoderBlock(nb_filter[1]*3+nb_filter[2], nb_filter[1])\n",
    "\n",
    "        self.conv0_4 = NestedDecoderBlock(nb_filter[0]*4+nb_filter[1], nb_filter[0])\n",
    "\n",
    "        self.final = nn.Conv2d(nb_filter[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x0_0 = self.conv0_0(input)\n",
    "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
    "\n",
    "        x0_0_att = self.attention1(g=self.up(x1_0), x=x0_0)\n",
    "        x0_1 = self.conv0_1(torch.cat([x0_0_att, self.up(x1_0)], dim=1))\n",
    "\n",
    "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
    "        x1_0_att = self.attention2(g=self.up(x2_0), x=x1_0)\n",
    "        x1_1 = self.conv1_1(torch.cat([x1_0_att, self.up(x2_0)], dim=1))\n",
    "\n",
    "        x0_1_att = self.attention1(g=self.up(x1_1), x=x0_1)\n",
    "        x0_2 = self.conv0_2(torch.cat([x0_0_att, x0_1_att, self.up(x1_1)], dim=1))\n",
    "\n",
    "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
    "        x2_0_att = self.attention3(g=self.up(x3_0), x=x2_0)\n",
    "        x2_1 = self.conv2_1(torch.cat([x2_0_att, self.up(x3_0)], dim=1))\n",
    "\n",
    "        x1_1_att = self.attention2(g=self.up(x2_1), x=x1_1)\n",
    "        x1_2 = self.conv1_2(torch.cat([x1_0_att, x1_1_att, self.up(x2_1)], dim=1))\n",
    "\n",
    "        x0_2_att = self.attention1(g=self.up(x1_2), x=x0_2)\n",
    "        x0_3 = self.conv0_3(torch.cat([x0_0_att, x0_1_att, x0_2_att, self.up(x1_2)], dim=1))\n",
    "\n",
    "        x4_0 = self.conv4_0(self.pool(x3_0))\n",
    "        x3_0_att = self.attention4(g=self.up(x4_0), x=x3_0)\n",
    "        x3_1 = self.conv3_1(torch.cat([x3_0_att, self.up(x4_0)], dim=1))\n",
    "\n",
    "        x2_1_att = self.attention3(g=self.up(x3_1), x=x2_1)\n",
    "        x2_2 = self.conv2_2(torch.cat([x2_0_att, x2_1_att, self.up(x3_1)], dim=1))\n",
    "\n",
    "        x1_2_att = self.attention2(g=self.up(x2_2), x=x1_2)\n",
    "        x1_3 = self.conv1_3(torch.cat([x1_0_att, x1_1_att, x1_2_att, self.up(x2_2)], dim=1))\n",
    "\n",
    "        x0_3_att = self.attention1(g=self.up(x1_3), x=x0_3)\n",
    "        x0_4 = self.conv0_4(torch.cat([x0_0_att, x0_1_att, x0_2_att, x0_3_att, self.up(x1_3)], dim=1))\n",
    "\n",
    "        output = self.final(x0_4)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c54266-1957-486e-a441-37d4f86bdc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Четвертая версия Attention U-Net++ с механизмом внимания (увеличение глубины нейронной сети)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        out = self.conv(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "class NestedDecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        inputs = [x for x in inputs if x is not None]\n",
    "        x = torch.cat(inputs, dim=1)\n",
    "        residual = self.shortcut(x)\n",
    "        out = self.conv(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class AttentionUnetPlusPlus4(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=8):\n",
    "        super().__init__()\n",
    "\n",
    "        nb_filter = [32, 64, 128, 256, 512]\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv0_0 = EncoderBlock(in_channels, nb_filter[0])\n",
    "        self.conv1_0 = EncoderBlock(nb_filter[0], nb_filter[1])\n",
    "        self.conv2_0 = EncoderBlock(nb_filter[1], nb_filter[2])\n",
    "        self.conv3_0 = EncoderBlock(nb_filter[2], nb_filter[3])\n",
    "        self.conv4_0 = EncoderBlock(nb_filter[3], nb_filter[4])\n",
    "\n",
    "        # Блоки внимания\n",
    "        self.attention1 = AttentionBlock(F_g=nb_filter[1], F_l=nb_filter[0], F_int=nb_filter[0]//2)\n",
    "        self.attention2 = AttentionBlock(F_g=nb_filter[2], F_l=nb_filter[1], F_int=nb_filter[1]//2)\n",
    "        self.attention3 = AttentionBlock(F_g=nb_filter[3], F_l=nb_filter[2], F_int=nb_filter[2]//2)\n",
    "        self.attention4 = AttentionBlock(F_g=nb_filter[4], F_l=nb_filter[3], F_int=nb_filter[3]//2)\n",
    "\n",
    "        self.conv0_1 = NestedDecoderBlock(nb_filter[0]+nb_filter[1], nb_filter[0])\n",
    "        self.conv1_1 = NestedDecoderBlock(nb_filter[1]+nb_filter[2], nb_filter[1])\n",
    "        self.conv2_1 = NestedDecoderBlock(nb_filter[2]+nb_filter[3], nb_filter[2])\n",
    "        self.conv3_1 = NestedDecoderBlock(nb_filter[3]+nb_filter[4], nb_filter[3])\n",
    "\n",
    "        self.conv0_2 = NestedDecoderBlock(nb_filter[0]*2+nb_filter[1], nb_filter[0])\n",
    "        self.conv1_2 = NestedDecoderBlock(nb_filter[1]*2+nb_filter[2], nb_filter[1])\n",
    "        self.conv2_2 = NestedDecoderBlock(nb_filter[2]*2+nb_filter[3], nb_filter[2])\n",
    "\n",
    "        self.conv0_3 = NestedDecoderBlock(nb_filter[0]*3+nb_filter[1], nb_filter[0])\n",
    "        self.conv1_3 = NestedDecoderBlock(nb_filter[1]*3+nb_filter[2], nb_filter[1])\n",
    "\n",
    "        self.conv0_4 = NestedDecoderBlock(nb_filter[0]*4+nb_filter[1], nb_filter[0])\n",
    "\n",
    "        self.final = nn.Conv2d(nb_filter[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Этап энкодера\n",
    "        x0_0 = self.conv0_0(input)\n",
    "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
    "\n",
    "        # Внимание и декодер\n",
    "        x0_0_att = self.attention1(g=self.up(x1_0), x=x0_0)\n",
    "        x0_1 = self.conv0_1(torch.cat([x0_0_att, self.up(x1_0)], dim=1))\n",
    "\n",
    "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
    "        x1_0_att = self.attention2(g=self.up(x2_0), x=x1_0)\n",
    "        x1_1 = self.conv1_1(torch.cat([x1_0_att, self.up(x2_0)], dim=1))\n",
    "\n",
    "        x0_1_att = self.attention1(g=self.up(x1_1), x=x0_1)\n",
    "        x0_2 = self.conv0_2(torch.cat([x0_0_att, x0_1_att, self.up(x1_1)], dim=1))\n",
    "\n",
    "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
    "        x2_0_att = self.attention3(g=self.up(x3_0), x=x2_0)\n",
    "        x2_1 = self.conv2_1(torch.cat([x2_0_att, self.up(x3_0)], dim=1))\n",
    "\n",
    "        x1_1_att = self.attention2(g=self.up(x2_1), x=x1_1)\n",
    "        x1_2 = self.conv1_2(torch.cat([x1_0_att, x1_1_att, self.up(x2_1)], dim=1))\n",
    "\n",
    "        x0_2_att = self.attention1(g=self.up(x1_2), x=x0_2)\n",
    "        x0_3 = self.conv0_3(torch.cat([x0_0_att, x0_1_att, x0_2_att, self.up(x1_2)], dim=1))\n",
    "\n",
    "        x4_0 = self.conv4_0(self.pool(x3_0))\n",
    "        x3_0_att = self.attention4(g=self.up(x4_0), x=x3_0)\n",
    "        x3_1 = self.conv3_1(torch.cat([x3_0_att, self.up(x4_0)], dim=1))\n",
    "\n",
    "        x2_1_att = self.attention3(g=self.up(x3_1), x=x2_1)\n",
    "        x2_2 = self.conv2_2(torch.cat([x2_0_att, x2_1_att, self.up(x3_1)], dim=1))\n",
    "\n",
    "        x1_2_att = self.attention2(g=self.up(x2_2), x=x1_2)\n",
    "        x1_3 = self.conv1_3(torch.cat([x1_0_att, x1_1_att, x1_2_att, self.up(x2_2)], dim=1))\n",
    "\n",
    "        x0_3_att = self.attention1(g=self.up(x1_3), x=x0_3)\n",
    "        x0_4 = self.conv0_4(torch.cat([x0_0_att, x0_1_att, x0_2_att, x0_3_att, self.up(x1_3)], dim=1))\n",
    "\n",
    "        output = self.final(x0_4)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada5fcf5-e10f-481f-b506-3b494d032711",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DeepLabV3+ с экстрактором признаков MobileNetV2 (модифицированной для одноканальных изображений)\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, rates=[6, 12, 18]):\n",
    "        super(ASPP, self).__init__()\n",
    "        self.aspp1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.aspp2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=rates[0], dilation=rates[0], bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.aspp3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=rates[1], dilation=rates[1], bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.aspp4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=rates[2], dilation=rates[2], bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.global_avg_pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv1 = nn.Conv2d(out_channels * 5, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        size = x.shape[2:]\n",
    "        aspp1 = self.aspp1(x)\n",
    "        aspp2 = self.aspp2(x)\n",
    "        aspp3 = self.aspp3(x)\n",
    "        aspp4 = self.aspp4(x)\n",
    "        global_avg = self.global_avg_pool(x)\n",
    "        global_avg = nn.functional.interpolate(global_avg, size=size, mode='bilinear', align_corners=True)\n",
    "        concat = torch.cat([aspp1, aspp2, aspp3, aspp4, global_avg], dim=1)\n",
    "        concat = self.conv1(concat)\n",
    "        concat = self.bn1(concat)\n",
    "        concat = self.relu(concat)\n",
    "        concat = self.dropout(concat)\n",
    "        return concat\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, low_level_in, low_level_out, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(low_level_in, low_level_out, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(low_level_out)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(low_level_out + out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x, low_level_feat):\n",
    "        low_level_feat = self.conv1(low_level_feat)\n",
    "        low_level_feat = self.bn1(low_level_feat)\n",
    "        low_level_feat = self.relu(low_level_feat)\n",
    "        \n",
    "        x = nn.functional.interpolate(x, size=low_level_feat.shape[2:], mode='bilinear', align_corners=True)\n",
    "        x = torch.cat([x, low_level_feat], dim=1)\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class DeepLabV3Plus_MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=2, pretrained=True):\n",
    "        super(DeepLabV3Plus_MobileNetV2, self).__init__()\n",
    "        mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "        \n",
    "        #Модификация первой свёрточной слоя для 1 канала\n",
    "        first_conv = mobilenet.features[0][0]\n",
    "        if first_conv.in_channels != 1:\n",
    "            new_first_conv = nn.Conv2d(\n",
    "                1,\n",
    "                first_conv.out_channels,\n",
    "                kernel_size=first_conv.kernel_size,\n",
    "                stride=first_conv.stride,\n",
    "                padding=first_conv.padding,\n",
    "                bias=first_conv.bias is not None\n",
    "            )\n",
    "            #Инициализация новых весов путем среднего значения по каналам\n",
    "            new_first_conv.weight.data = first_conv.weight.data.mean(dim=1, keepdim=True)\n",
    "            mobilenet.features[0][0] = new_first_conv\n",
    "        \n",
    "        #Извлечение необходимых слоёв\n",
    "        self.backbone = create_feature_extractor(\n",
    "            mobilenet, \n",
    "            return_nodes={\n",
    "                'features.18': 'high_level',  #Последний слой MobileNetV2\n",
    "                'features.3': 'low_level'     #Более ранний слой с 24 каналами для декодера\n",
    "            }\n",
    "        )\n",
    "        self.aspp = ASPP(in_channels=1280, out_channels=256)\n",
    "        self.decoder = Decoder(low_level_in=24, low_level_out=48, out_channels=256)\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        high_level = features['high_level']\n",
    "        low_level = features['low_level']\n",
    "        \n",
    "        aspp_out = self.aspp(high_level)\n",
    "        decoder_out = self.decoder(aspp_out, low_level)\n",
    "        out = self.final_conv(decoder_out)\n",
    "        out = nn.functional.interpolate(out, size=x.shape[2:], mode='bilinear', align_corners=True)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fa1609-5931-4806-b975-ac9d8575cf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Использование CBAM-attention (Convolutional Block Attention Module) \n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, reduction_ratio=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        #Channel Attention Module\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels // reduction_ratio, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels // reduction_ratio, channels, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid_channel = nn.Sigmoid()\n",
    "        \n",
    "        #Spatial Attention Module\n",
    "        self.conv_spatial = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)\n",
    "        self.sigmoid_spatial = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Channel Attention\n",
    "        avg_out = self.avg_pool(x)\n",
    "        max_out = self.max_pool(x)\n",
    "        avg_out = self.fc(avg_out)\n",
    "        max_out = self.fc(max_out)\n",
    "        channel_attn = self.sigmoid_channel(avg_out + max_out)\n",
    "        x = x * channel_attn\n",
    "        \n",
    "        #Spatial Attention\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        spatial_attn = torch.cat([avg_out, max_out], dim=1)\n",
    "        spatial_attn = self.conv_spatial(spatial_attn)\n",
    "        spatial_attn = self.sigmoid_spatial(spatial_attn)\n",
    "        x = x * spatial_attn\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, rates=[2, 4, 6]):\n",
    "        super(ASPP, self).__init__()\n",
    "        self.aspp1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.aspp2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=rates[0], dilation=rates[0], bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.aspp3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=rates[1], dilation=rates[1], bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.aspp4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=rates[2], dilation=rates[2], bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.global_avg_pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv1 = nn.Conv2d(out_channels * 5, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        #Добавление CBAM\n",
    "        self.cbam = CBAM(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        size = x.shape[2:]\n",
    "        aspp1 = self.aspp1(x)\n",
    "        aspp2 = self.aspp2(x)\n",
    "        aspp3 = self.aspp3(x)\n",
    "        aspp4 = self.aspp4(x)\n",
    "        global_avg = self.global_avg_pool(x)\n",
    "        global_avg = F.interpolate(global_avg, size=size, mode='bilinear', align_corners=True)\n",
    "        concat = torch.cat([aspp1, aspp2, aspp3, aspp4, global_avg], dim=1)\n",
    "        concat = self.conv1(concat)\n",
    "        concat = self.bn1(concat)\n",
    "        concat = self.relu(concat)\n",
    "        concat = self.dropout(concat)\n",
    "        \n",
    "        #Применение CBAM\n",
    "        concat = self.cbam(concat)\n",
    "        \n",
    "        return concat\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, low_level_in, low_level_out, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(low_level_in, low_level_out, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(low_level_out)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(low_level_out + out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.cbam = CBAM(out_channels)\n",
    "        \n",
    "    def forward(self, x, low_level_feat):\n",
    "        low_level_feat = self.conv1(low_level_feat)\n",
    "        low_level_feat = self.bn1(low_level_feat)\n",
    "        low_level_feat = self.relu(low_level_feat)\n",
    "        \n",
    "        x = F.interpolate(x, size=low_level_feat.shape[2:], mode='bilinear', align_corners=True)\n",
    "        x = torch.cat([x, low_level_feat], dim=1)\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.cbam(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class DeepLabV3Plus_MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=2, pretrained=True):\n",
    "        super(DeepLabV3Plus_MobileNetV2, self).__init__()\n",
    "        mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "        \n",
    "        self.adapter = nn.Sequential( \n",
    "            nn.Conv2d(1, 3, kernel_size=3, padding = 1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.backbone = create_feature_extractor(\n",
    "            mobilenet, \n",
    "            return_nodes={\n",
    "                'features.18': 'high_level', \n",
    "                'features.3': 'low_level' \n",
    "            }\n",
    "        )\n",
    "        self.aspp = ASPP(in_channels=1280, out_channels=256)\n",
    "        self.decoder = Decoder(low_level_in=24, low_level_out=48, out_channels=256)\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.adapter(x)\n",
    "        features = self.backbone(x)\n",
    "        high_level = features['high_level']\n",
    "        low_level = features['low_level']\n",
    "        \n",
    "        aspp_out = self.aspp(high_level)\n",
    "        decoder_out = self.decoder(aspp_out, low_level)\n",
    "        out = self.final_conv(decoder_out)\n",
    "        out = F.interpolate(out, size=x.shape[2:], mode='bilinear', align_corners=True)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7448fff7-f1b7-40c3-b06c-2cc4e62a513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Использование EfficientNetB0 вместо MobileNetV2\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, reduction_ratio=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        \n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels // reduction_ratio, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels // reduction_ratio, channels, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid_channel = nn.Sigmoid()\n",
    "        \n",
    "        self.conv_spatial = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)\n",
    "        self.sigmoid_spatial = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = self.avg_pool(x)\n",
    "        max_out = self.max_pool(x)\n",
    "        avg_out = self.fc(avg_out)\n",
    "        max_out = self.fc(max_out)\n",
    "        channel_attn = self.sigmoid_channel(avg_out + max_out)\n",
    "        x = x * channel_attn\n",
    "        \n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        spatial_attn = torch.cat([avg_out, max_out], dim=1)\n",
    "        spatial_attn = self.conv_spatial(spatial_attn)\n",
    "        spatial_attn = self.sigmoid_spatial(spatial_attn)\n",
    "        x = x * spatial_attn\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, rates=[2, 4, 6]):\n",
    "        super(ASPP, self).__init__()\n",
    "        self.aspp1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.aspp2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=rates[0], dilation=rates[0], bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.aspp3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=rates[1], dilation=rates[1], bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.aspp4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=rates[2], dilation=rates[2], bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.global_avg_pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv1 = nn.Conv2d(out_channels * 5, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.cbam = CBAM(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        size = x.shape[2:]\n",
    "        aspp1 = self.aspp1(x)\n",
    "        aspp2 = self.aspp2(x)\n",
    "        aspp3 = self.aspp3(x)\n",
    "        aspp4 = self.aspp4(x)\n",
    "        global_avg = self.global_avg_pool(x)\n",
    "        global_avg = F.interpolate(global_avg, size=size, mode='bilinear', align_corners=True)\n",
    "        concat = torch.cat([aspp1, aspp2, aspp3, aspp4, global_avg], dim=1)\n",
    "        concat = self.conv1(concat)\n",
    "        concat = self.bn1(concat)\n",
    "        concat = self.relu(concat)\n",
    "        concat = self.dropout(concat)\n",
    "        \n",
    "        concat = self.cbam(concat)\n",
    "        \n",
    "        return concat\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, low_level_in, low_level_out, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(low_level_in, low_level_out, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(low_level_out)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(low_level_out + out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.cbam = CBAM(out_channels)\n",
    "        \n",
    "    def forward(self, x, low_level_feat):\n",
    "        low_level_feat = self.conv1(low_level_feat)\n",
    "        low_level_feat = self.bn1(low_level_feat)\n",
    "        low_level_feat = self.relu(low_level_feat)\n",
    "        \n",
    "        x = F.interpolate(x, size=low_level_feat.shape[2:], mode='bilinear', align_corners=True)\n",
    "        x = torch.cat([x, low_level_feat], dim=1)\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.cbam(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class DeepLabV3Plus_EfficientNetB0(nn.Module):\n",
    "    def __init__(self, num_classes=2, pretrained=True):\n",
    "        super(DeepLabV3Plus_EfficientNetB0, self).__init__()\n",
    "        efficientnet = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "        \n",
    "        self.adapter = nn.Sequential( \n",
    "            nn.Conv2d(1, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        #Извлечение необходимых слоёв из EfficientNet_B0\n",
    "        self.backbone = create_feature_extractor(\n",
    "            efficientnet, \n",
    "            return_nodes={\n",
    "                'features.8': 'high_level',  #Последний слой перед классификатором\n",
    "                'features.2': 'low_level'    #Ранний слой с 24 каналами для декодера\n",
    "            }\n",
    "        )\n",
    "        self.aspp = ASPP(in_channels=1280, out_channels=256, rates=[1, 2, 3])  #Измененные rates для более узкого контекста (или меньших изображений)\n",
    "        self.decoder = Decoder(low_level_in=24, low_level_out=48, out_channels=256)\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.adapter(x)\n",
    "        features = self.backbone(x)\n",
    "        high_level = features['high_level']\n",
    "        low_level = features['low_level']\n",
    "        \n",
    "        aspp_out = self.aspp(high_level)\n",
    "        decoder_out = self.decoder(aspp_out, low_level)\n",
    "        out = self.final_conv(decoder_out)\n",
    "        out = F.interpolate(out, size=x.shape[2:], mode='bilinear', align_corners=True)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8e837f-55e3-4e31-8c59-ce5f30ce53e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Использование архитектуры MedT - Medical Transformer\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, stride=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding, stride=stride),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, in_channels, num_heads=8, num_layers=4):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [nn.TransformerEncoderLayer(d_model=in_channels, nhead=num_heads) for _ in range(num_layers)]\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.view(B, C, -1).permute(2, 0, 1)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x.permute(1, 2, 0).view(B, C, H, W)\n",
    "        return x\n",
    "\n",
    "class MedT(nn.Module):\n",
    "    def __init__(self, img_size=128, in_channels=1, num_classes=8):\n",
    "        super(MedT, self).__init__()\n",
    "        self.encoder1 = ConvBlock(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.encoder2 = ConvBlock(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.encoder3 = ConvBlock(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.encoder4 = ConvBlock(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        \n",
    "        #Transformer block\n",
    "        self.transformer = TransformerBlock(512, num_heads=8, num_layers=4)\n",
    "        \n",
    "        self.upconv4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder4 = ConvBlock(768, 256) \n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder3 = ConvBlock(384, 128)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder2 = ConvBlock(192, 64) \n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.decoder1 = ConvBlock(96, 32) \n",
    "        \n",
    "        self.conv_last = nn.Conv2d(32, num_classes, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.encoder1(x)       \n",
    "        x_pool1 = self.pool1(x1)    \n",
    "        \n",
    "        x2 = self.encoder2(x_pool1) \n",
    "        x_pool2 = self.pool2(x2)    \n",
    "        \n",
    "        x3 = self.encoder3(x_pool2)   \n",
    "        x_pool3 = self.pool3(x3)   \n",
    "        \n",
    "        x4 = self.encoder4(x_pool3)\n",
    "        x_pool4 = self.pool4(x4)   \n",
    "        \n",
    "        #Transformer\n",
    "        x_transformed = self.transformer(x_pool4)\n",
    "        \n",
    "        x_up4 = self.upconv4(x_transformed)   \n",
    "        x_cat4 = torch.cat([x_up4, x4], dim=1)\n",
    "        x_dec4 = self.decoder4(x_cat4)        \n",
    "        \n",
    "        x_up3 = self.upconv3(x_dec4)          \n",
    "        x_cat3 = torch.cat([x_up3, x3], dim=1)\n",
    "        x_dec3 = self.decoder3(x_cat3)        \n",
    "        \n",
    "        x_up2 = self.upconv2(x_dec3)          \n",
    "        x_cat2 = torch.cat([x_up2, x2], dim=1)  \n",
    "        x_dec2 = self.decoder2(x_cat2)            \n",
    "        \n",
    "        x_up1 = self.upconv1(x_dec2)           \n",
    "        x_cat1 = torch.cat([x_up1, x1], dim=1)     \n",
    "        x_dec1 = self.decoder1(x_cat1)          \n",
    "        \n",
    "        output = self.conv_last(x_dec1)           \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c85e68-24de-43b8-b277-d02150c1a8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer U-Net\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(channels)-1):\n",
    "            self.layers.append(nn.Sequential(\n",
    "                ConvBlock(channels[i], channels[i+1]),\n",
    "                ConvBlock(channels[i+1], channels[i+1])\n",
    "            ))\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            features.append(x)\n",
    "            x = F.max_pool2d(x, kernel_size=2)\n",
    "        return x, features\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(channels)-1):\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.ConvTranspose2d(channels[i], channels[i+1], kernel_size=2, stride=2),\n",
    "                ConvBlock(channels[i], channels[i+1]),\n",
    "                ConvBlock(channels[i+1], channels[i+1])\n",
    "            ))\n",
    "    def forward(self, x, features):\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i][0](x)\n",
    "            x = torch.cat([x, features[-(i+1)]], dim=1)\n",
    "            x = self.layers[i][1](x)\n",
    "            x = self.layers[i][2](x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim*4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim*4, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "    def forward(self, x):\n",
    "        x_res = x\n",
    "        x = self.norm1(x)\n",
    "        x, _ = self.attn(x, x, x)\n",
    "        x = x_res + x\n",
    "        x_res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = x_res + x\n",
    "        return x\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, img_size=128, in_channels=1, num_classes=8, base_channels=64, num_heads=8, num_layers=12, embed_dim=512, patch_size=8):\n",
    "        super(TransUNet, self).__init__()\n",
    "        self.encoder = Encoder([in_channels, base_channels, base_channels*2, base_channels*4, base_channels*8])\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            ConvBlock(base_channels*8, base_channels*16),\n",
    "            ConvBlock(base_channels*16, base_channels*16)\n",
    "        )\n",
    "        self.patch_embedding = nn.Conv2d(base_channels*16, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        num_patches = (img_size // (2**4 * patch_size))**2\n",
    "        \n",
    "        self.transformer = nn.Sequential(*[TransformerBlock(embed_dim, num_heads) for _ in range(num_layers)])\n",
    "        \n",
    "        self.proj_back = nn.ConvTranspose2d(embed_dim, base_channels*16, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "        self.decoder = Decoder([base_channels*16, base_channels*8, base_channels*4, base_channels*2, base_channels])\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(base_channels, num_classes, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x, features = self.encoder(x)\n",
    "        \n",
    "        x = self.bottleneck(x)\n",
    "        \n",
    "        B, C, H, W = x.shape\n",
    "        x = self.patch_embedding(x).flatten(2).permute(2, 0, 1)\n",
    "        \n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        x = x.permute(1, 2, 0).view(B, -1, H // self.patch_embedding.kernel_size[0], W // self.patch_embedding.kernel_size[0])\n",
    "        x = self.proj_back(x)\n",
    "        \n",
    "        x = self.decoder(x, features)\n",
    "        \n",
    "        x = self.final_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6046ed4-f58a-47a9-b9dd-3dd07046848b",
   "metadata": {},
   "source": [
    "Прочие архитектуры, основанные на vision-transformers здесь не привожу, поскольку они не показали высокой эффективности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f71dfdd-b91c-49a2-9cc8-2f84bd32b169",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функционалы потерь\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        num_classes = inputs.size(1)  #Число классов (4)\n",
    "\n",
    "        inputs = F.softmax(inputs, dim=1) \n",
    "\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=num_classes).permute(0, 3, 1, 2).float()\n",
    "\n",
    "        intersection = torch.sum(inputs * targets_one_hot, dim=(2, 3)) \n",
    "        cardinality = torch.sum(inputs + targets_one_hot, dim=(2, 3))\n",
    "\n",
    "        dice_loss = 1 - (2. * intersection + self.eps) / (cardinality + self.eps)\n",
    "        loss = dice_loss.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=None, eps=1e-6):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha \n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        num_classes = inputs.size(1)\n",
    "\n",
    "        inputs_soft = F.softmax(inputs, dim=1) + self.eps \n",
    "\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=num_classes).permute(0, 3, 1, 2).float()  \n",
    "\n",
    "        ce_loss = -targets_one_hot * torch.log(inputs_soft)  \n",
    "\n",
    "        focal_loss = ce_loss * ((1 - inputs_soft) ** self.gamma)\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha = torch.tensor(self.alpha).to(inputs.device) \n",
    "            focal_loss = alpha.view(1, -1, 1, 1) * focal_loss \n",
    "\n",
    "        loss = focal_loss.mean() \n",
    "\n",
    "        return loss\n",
    "\n",
    "class IOULoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super(IOULoss, self).__init__()\n",
    "        self.eps = eps \n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        num_classes = inputs.size(1) \n",
    "\n",
    "        inputs = F.softmax(inputs, dim=1)\n",
    "\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=num_classes).permute(0, 3, 1, 2).float()\n",
    "\n",
    "        intersection = torch.sum(inputs * targets_one_hot, dim=(2, 3)) \n",
    "        union = torch.sum(inputs + targets_one_hot - inputs * targets_one_hot, dim=(2, 3))\n",
    "\n",
    "        iou = (intersection + self.eps) / (union + self.eps)\n",
    "\n",
    "        iou_loss = 1 - iou\n",
    "        loss = iou_loss.mean()  \n",
    "\n",
    "        return loss\n",
    "\n",
    "class HybridLoss(nn.Module):\n",
    "    def __init__(self, weight=None):\n",
    "        super(HybridLoss, self).__init__()\n",
    "        self.dice_loss = DiceLoss()\n",
    "        self.focal_loss = FocalLoss()\n",
    "        self.iou_loss = IOULoss()\n",
    "        self.weight = weight  #Список весов для каждой функции потерь\n",
    "\n",
    "        if self.weight is None:\n",
    "            #Если веса не заданы, используем равные веса\n",
    "            self.weight = [1/3, 1/3, 1/3]\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        loss_dice = self.dice_loss(inputs, targets)\n",
    "        loss_focal = self.focal_loss(inputs, targets)\n",
    "        loss_iou = self.iou_loss(inputs, targets)\n",
    "\n",
    "        #Комбинируем потери с заданными весами\n",
    "        loss = self.weight[0] * loss_dice + self.weight[1] * loss_focal + self.weight[2] * loss_iou\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ed50049-8d97-4708-9c6a-f06c77b8ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Метрики качества\n",
    "class DiceCoefficient(nn.Module):\n",
    "    def __init__(self, num_classes, eps=1e-8):\n",
    "        super(DiceCoefficient, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, inputs, targets, use_argmax=True):\n",
    "        if use_argmax:\n",
    "            #Получаем предсказанные классы через argmax\n",
    "            preds = torch.argmax(inputs, dim=1)\n",
    "        else:\n",
    "            preds = inputs\n",
    "        \n",
    "        dice_scores = []\n",
    "\n",
    "        #Итерация по всем классам, кроме фона\n",
    "        for cls in range(1, self.num_classes):\n",
    "            pred_mask = (preds == cls).float() \n",
    "            target_mask = (targets == cls).float() \n",
    "\n",
    "            intersection = (pred_mask * target_mask).sum(dim=(1, 2))\n",
    "            pred_sum = pred_mask.sum(dim=(1, 2))\n",
    "            target_sum = target_mask.sum(dim=(1, 2))\n",
    "            union = pred_sum + target_sum\n",
    "\n",
    "            dice = (2 * intersection + self.eps) / (union + self.eps)\n",
    "\n",
    "            dice_scores.append(dice)\n",
    "\n",
    "        dice_scores = torch.stack(dice_scores, dim=1)\n",
    "\n",
    "        dice_mean = dice_scores.mean()\n",
    "\n",
    "        return dice_mean\n",
    "\n",
    "class OneClassDiceCoefficient(nn.Module):\n",
    "    def __init__(self, num_classes, class_num, eps=1e-8):\n",
    "        super(OneClassDiceCoefficient, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.eps = eps\n",
    "        self.class_num = class_num\n",
    "\n",
    "    def forward(self, inputs, targets, use_argmax=True):\n",
    "        if use_argmax:\n",
    "            preds = torch.argmax(inputs, dim=1)  \n",
    "        else:\n",
    "            preds = inputs\n",
    "\n",
    "        cls = self.class_num\n",
    "\n",
    "        pred_mask = (preds == cls).float()\n",
    "        target_mask = (targets == cls).float() \n",
    "\n",
    "        intersection = (pred_mask * target_mask).sum(dim=(1, 2))\n",
    "        pred_sum = pred_mask.sum(dim=(1, 2)) \n",
    "        target_sum = target_mask.sum(dim=(1, 2))\n",
    "        union = pred_sum + target_sum\n",
    "\n",
    "        dice = torch.where(union > 0, (2 * intersection + self.eps) / (union + self.eps), torch.zeros_like(union).float())\n",
    "\n",
    "        dice_mean = dice.mean()\n",
    "        return dice_mean\n",
    "\n",
    "class IOU(nn.Module):\n",
    "    def __init__(self, num_classes, eps=1e-8):\n",
    "        super(IOU, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, inputs, targets, use_argmax=True):\n",
    "        if use_argmax:\n",
    "            preds = torch.argmax(inputs, dim=1) \n",
    "        else:\n",
    "            preds = inputs\n",
    "\n",
    "        iou_list = []\n",
    "        for cls in range(1, self.num_classes):\n",
    "            pred_cls = (preds == cls).float()\n",
    "            target_cls = (targets == cls).float() \n",
    "\n",
    "            intersection = torch.sum(pred_cls * target_cls, dim=(1, 2))\n",
    "            union = torch.sum(pred_cls + target_cls - pred_cls * target_cls, dim=(1, 2)) \n",
    "\n",
    "            iou = (intersection + self.eps) / (union + self.eps) \n",
    "            iou_list.append(iou)\n",
    "            \n",
    "        iou = torch.stack(iou_list, dim=1) \n",
    "\n",
    "        iou_mean = iou.mean()\n",
    "\n",
    "        return iou_mean\n",
    "\n",
    "class OneClassIOU(nn.Module):\n",
    "    def __init__(self, num_classes, class_num, eps=1e-8):\n",
    "        super(OneClassIOU, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.eps = eps\n",
    "        self.class_num = class_num\n",
    "\n",
    "    def forward(self, inputs, targets, use_argmax=True):\n",
    "        if use_argmax:\n",
    "            preds = torch.argmax(inputs, dim=1) \n",
    "        else:\n",
    "            preds = inputs\n",
    "\n",
    "        cls = self.class_num\n",
    "        pred_cls = (preds == cls).float() \n",
    "        target_cls = (targets == cls).float() \n",
    "\n",
    "        intersection = torch.sum(pred_cls * target_cls, dim=(1, 2))\n",
    "        union = torch.sum(pred_cls + target_cls - pred_cls * target_cls, dim=(1, 2))\n",
    "\n",
    "        iou = torch.where(union > 0, (intersection + self.eps) / (union + self.eps), torch.zeros_like(union).float())\n",
    "\n",
    "        iou_mean = iou.mean()\n",
    "        \n",
    "        return iou_mean\n",
    "\n",
    "class PixelAccuracy(nn.Module):\n",
    "    def __init__(self, ignore_index=None):\n",
    "        super(PixelAccuracy, self).__init__()\n",
    "        self.ignore_index = ignore_index  # Индекс класса, который следует игнорировать (фон)\n",
    "\n",
    "    def forward(self, inputs, targets, use_argmax=True):\n",
    "        if use_argmax:\n",
    "            _, preds = torch.max(inputs, dim=1) \n",
    "        else:\n",
    "            preds = inputs\n",
    "\n",
    "        if self.ignore_index is not None:\n",
    "            mask = targets != self.ignore_index\n",
    "            correct = (preds[mask] == targets[mask]).sum().float()\n",
    "            total = mask.sum().float()\n",
    "        else:\n",
    "            correct = (preds == targets).sum().float()\n",
    "            total = targets.numel()\n",
    "\n",
    "        accuracy = correct / (total + 1e-8)\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "class Precision(nn.Module):\n",
    "    def __init__(self, num_classes, eps=1e-8):\n",
    "        super(Precision, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, inputs, targets, use_argmax=True):\n",
    "        if use_argmax:\n",
    "            _, preds = torch.max(inputs, dim=1) \n",
    "        else:\n",
    "            preds = inputs\n",
    "\n",
    "        precision = []\n",
    "        for cls in range(1, self.num_classes):\n",
    "            true_positive = ((preds == cls) & (targets == cls)).float().view(preds.size(0), -1).sum(dim=1) \n",
    "            predicted_positive = (preds == cls).float().view(preds.size(0), -1).sum(dim=1) \n",
    "            precision_cls = (true_positive + self.eps) / (predicted_positive + self.eps) \n",
    "            precision.append(precision_cls)\n",
    "\n",
    "        precision = torch.stack(precision, dim=1) \n",
    "        precision_per_object = precision.mean(dim=1) \n",
    "\n",
    "        precision_mean = precision_per_object.mean()\n",
    "\n",
    "        return precision_mean\n",
    "\n",
    "class OneClassPrecision(nn.Module):\n",
    "    def __init__(self, num_classes, class_num, eps=1e-8):\n",
    "        super(OneClassPrecision, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.eps = eps  \n",
    "        self.class_num = class_num\n",
    "\n",
    "    def forward(self, inputs, targets, use_argmax=True):\n",
    "        if use_argmax:\n",
    "            _, preds = torch.max(inputs, dim=1) \n",
    "        else:\n",
    "            preds = inputs\n",
    "\n",
    "        true_positive = ((preds == self.class_num) & (targets == self.class_num)).float().view(preds.size(0), -1).sum(dim=1) \n",
    "        predicted_positive = (preds == self.class_num).float().view(preds.size(0), -1).sum(dim=1)\n",
    "        precision_cls = (true_positive + self.eps) / (predicted_positive + self.eps) \n",
    "\n",
    "        precision_mean = precision_cls.mean()\n",
    "\n",
    "        return precision_mean\n",
    "\n",
    "class Recall(nn.Module):\n",
    "    def __init__(self, num_classes, eps=1e-8):\n",
    "        super(Recall, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.eps = eps \n",
    "\n",
    "    def forward(self, inputs, targets, use_argmax=True):\n",
    "        if use_argmax:\n",
    "            _, preds = torch.max(inputs, dim=1) \n",
    "        else:\n",
    "            preds = inputs\n",
    "\n",
    "        recall = []\n",
    "        for cls in range(1, self.num_classes):\n",
    "            true_positive = ((preds == cls) & (targets == cls)).float().view(preds.size(0), -1).sum(dim=1)\n",
    "            actual_positive = (targets == cls).float().view(targets.size(0), -1).sum(dim=1)\n",
    "            recall_cls = (true_positive + self.eps) / (actual_positive + self.eps) \n",
    "            recall.append(recall_cls)\n",
    "        \n",
    "        recall = torch.stack(recall, dim=1)\n",
    "        recall_per_object = recall.mean(dim=1) \n",
    "\n",
    "        recall_mean = recall_per_object.mean()\n",
    "\n",
    "        return recall_mean\n",
    "\n",
    "class OneClassRecall(nn.Module):\n",
    "    def __init__(self, num_classes, class_num, eps=1e-8):\n",
    "        super(OneClassRecall, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.eps = eps  \n",
    "        self.class_num = class_num\n",
    "\n",
    "    def forward(self, inputs, targets, use_argmax=True):\n",
    "        if use_argmax:\n",
    "            _, preds = torch.max(inputs, dim=1)  \n",
    "        else:\n",
    "            preds = inputs\n",
    "\n",
    "        true_positive = ((preds == self.class_num) & (targets == self.class_num)).float().view(preds.size(0), -1).sum(dim=1) \n",
    "        actual_positive = (targets == self.class_num).float().view(targets.size(0), -1).sum(dim=1) \n",
    "\n",
    "        recall_cls = torch.where(actual_positive > 0, (true_positive + self.eps) / (actual_positive + self.eps), torch.zeros_like(actual_positive).float())\n",
    "\n",
    "        recall_mean = recall_cls.mean()\n",
    "\n",
    "        return recall_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c46d5891-ea5d-483f-a773-6cd521ab94a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'mps'\n",
    "BATCH_SIZE = 32    \n",
    "\n",
    "def train_segmentation_model(model, optimizer, criterion, train_transforms, val_transforms, header, metrics, n_epoch = 101):\n",
    "    \"\"\"\n",
    "    model - модель (экземпляр) для обучения\n",
    "    optimizer - оптимизатор\n",
    "    criterion - функционал ошибки\n",
    "    train_transforms - аугментации для обучения\n",
    "    val_transforms - аугментации для валидации/теста (обычно только Resize) \n",
    "    header - описание эксперимента, нужное для логирования (логи в segmentation_logs.txt) \n",
    "    metrics - метрики для логирования\n",
    "    n_epoch - максимальное количество эпох обучения (под эпохой подразумевается не полный прогон всех данных, а только size из CustomDataset из-за ограниченных вычислительных ресурсов)\n",
    "    \"\"\"\n",
    "    with open('segmentation_logs.txt', 'a') as file:\n",
    "       file.write(header + '\\n')\n",
    "    max_acc = 0\n",
    "    min_loss = 100\n",
    "    losses_train, losses_val = [], []\n",
    "\n",
    "    train_dataset = CustomDataset('subset0', 'seg-lungs-LUNA16', 960, 20, 10, train_transforms)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_dataset = CustomDataset('subset1', 'seg-lungs-LUNA16', 384, 3, 10, val_transforms)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True) \n",
    "\n",
    "    current_metrics = [0] * (2 * len(metrics))\n",
    "    \n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        model.train()\n",
    "\n",
    "        current_metrics = [0] * (2 * len(metrics))\n",
    "        \n",
    "        train_loss = 0\n",
    "        for inputs, masks in tqdm(train_dataloader):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            masks = masks.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, masks.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            for i in range(len(metrics)):\n",
    "                current_metrics[i] += metrics[i][0](outputs, masks.long()).item()\n",
    "\n",
    "        losses_train.append(train_loss / len(train_dataloader))\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        val_pixel_acc = 0\n",
    "        mask_true0 = 0\n",
    "        mask_true1 = 0\n",
    "        mask_true2 = 0\n",
    "        mask_true3 = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, masks in tqdm(val_dataloader):\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                masks = masks.to(DEVICE)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                loss = criterion(outputs, masks.long())\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                for i in range(inputs.shape[0]):\n",
    "                    mask_true0 += (masks[i] == 0).cpu().numpy().sum()\n",
    "                    mask_true1 += (masks[i] == 1).cpu().numpy().sum()\n",
    "                    mask_true2 += (masks[i] == 2).cpu().numpy().sum()\n",
    "                    mask_true3 += (masks[i] == 3).cpu().numpy().sum()\n",
    "                  \n",
    "                for i in range(len(metrics)):\n",
    "                    current_metrics[i + len(metrics)] += metrics[i][0](outputs, masks.long()).item()\n",
    "\n",
    "        print(mask_true0, mask_true1, mask_true2, mask_true3) #отладочный вывод (можно использовать для баланса классов)\n",
    "        losses_val.append(val_loss / len(val_dataloader))\n",
    "                \n",
    "        for i in range(len(metrics)):\n",
    "            current_metrics[i] = current_metrics[i] / len(train_dataloader)\n",
    "            current_metrics[i + len(metrics)] = current_metrics[i + len(metrics)] / len(val_dataloader)\n",
    "\n",
    "        out_string = f'Iteration: {epoch}\\ntrain loss: {losses_train[-1]}\\n'\n",
    "        for i in range(len(metrics)):\n",
    "            out_string += f'train {metrics[i][1]}: {current_metrics[i]}\\n'\n",
    "            \n",
    "        out_string += f'test loss: {losses_val[-1]}\\n'\n",
    "        for i in range(len(metrics)):\n",
    "            out_string += f'test {metrics[i][1]}: {current_metrics[i + len(metrics)]}\\n'\n",
    "\n",
    "        with open('segmentation_logs.txt', 'a') as file:\n",
    "           file.write(out_string + '\\n')    \n",
    "\n",
    "        torch.save({\n",
    "             'model_state_dict': model.state_dict(),\n",
    "             'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, f'mini_unet_{epoch}.pth')\n",
    "        print(out_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6be19357-5fb3-4d86-b959-ed0f3811ae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = A.Compose([\n",
    "    A.Resize(128, 128),\n",
    "    #A.RandomResizedCrop(height=128, width=128, scale=(0.95, 1.0), ratio=(1.0, 1.0), p=1),\n",
    "    #A.Rotate(limit=5, p=0.7),\n",
    "    #A.GaussNoise(var_limit=(5.0, 20.0), p=0.1),\n",
    "    ToTensorV2()\n",
    "])\n",
    "val_transforms = A.Compose([\n",
    "    A.Resize(128, 128),\n",
    "    ToTensorV2()\n",
    "])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa21b2ed-d97a-48bc-9b75-c3b870f3f838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Обучение модели\n",
    "model = MiniUnet().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "metrics = [(PixelAccuracy(), 'pixel accuracy'), \n",
    "           (DiceCoefficient(4), 'mean dice'),\n",
    "           (OneClassDiceCoefficient(4, 1), 'right lung dice'),\n",
    "           (OneClassDiceCoefficient(4, 2), 'left lung dice'),\n",
    "           (OneClassDiceCoefficient(4, 3), 'trachea dice'),\n",
    "           (IOU(4), 'mean IOU'),\n",
    "           (OneClassIOU(4, 1), 'right lung IOU'),\n",
    "           (OneClassIOU(4, 2), 'left lung IOU'),\n",
    "           (OneClassIOU(4, 3), 'trachea IOU'),\n",
    "           (Precision(4), 'mean precision'),\n",
    "           (OneClassPrecision(4, 1), 'right lung precision'),\n",
    "           (OneClassPrecision(4, 2), 'left lung precision'),\n",
    "           (OneClassPrecision(4, 3), 'trachea precision'),\n",
    "           (Recall(4), 'mean recall'),\n",
    "           (OneClassRecall(4, 1), 'right lung recall'),\n",
    "           (OneClassRecall(4, 2), 'left lung recall'),\n",
    "           (OneClassRecall(4, 3), 'trachea recall')]\n",
    "train_segmentation_model(model, optimizer, criterion, train_transforms, val_transforms, 'Mini U-Net, cross-entropy loss, adam optimizer, batch size = 32, only resize to 128x128 augmentations, datasets = (20, 10), (3, 8)', metrics, n_epoch = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea103e-878d-46ad-a399-854cb35446c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Пример для Lion-optimizer\n",
    "model = MiniUnet().to(DEVICE)\n",
    "optimizer = Lion(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "metrics = [(PixelAccuracy(), 'pixel accuracy'), \n",
    "           (DiceCoefficient(4), 'mean dice'),\n",
    "           (OneClassDiceCoefficient(4, 1), 'right lung dice'),\n",
    "           (OneClassDiceCoefficient(4, 2), 'left lung dice'),\n",
    "           (OneClassDiceCoefficient(4, 3), 'trachea dice'),\n",
    "           (IOU(4), 'mean IOU'),\n",
    "           (OneClassIOU(4, 1), 'right lung IOU'),\n",
    "           (OneClassIOU(4, 2), 'left lung IOU'),\n",
    "           (OneClassIOU(4, 3), 'trachea IOU'),\n",
    "           (Precision(4), 'mean precision'),\n",
    "           (OneClassPrecision(4, 1), 'right lung precision'),\n",
    "           (OneClassPrecision(4, 2), 'left lung precision'),\n",
    "           (OneClassPrecision(4, 3), 'trachea precision'),\n",
    "           (Recall(4), 'mean recall'),\n",
    "           (OneClassRecall(4, 1), 'right lung recall'),\n",
    "           (OneClassRecall(4, 2), 'left lung recall'),\n",
    "           (OneClassRecall(4, 3), 'trachea recall')]\n",
    "train_segmentation_model(model, optimizer, criterion, train_transforms, val_transforms, 'Mini U-Net, cross-entropy loss, lion optimizer, batch size = 32, only resize to 128x128 augmentations, datasets = (20, 10), (3, 8)', metrics, n_epoch = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b99f85-5982-419b-835b-81b9a205a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Визуальная проверка нейросети (устарело; требует переработки после оптимизации обучения нейронных сетей)\n",
    "accs = []\n",
    "DEVICE = 'mps'\n",
    "def show_input_output(model, name):\n",
    "    global accs\n",
    "    model.eval()\n",
    "    #Читаем изображение используя SimpleITK\n",
    "    image = sitk.ReadImage(os.path.join('subset1', name))\n",
    "    ground_true = sitk.ReadImage(os.path.join('seg-lungs-LUNA16', name))\n",
    "\n",
    "    for i in range(image.GetDepth()-1):\n",
    "        #Извлекаем заданный срез\n",
    "        input_image = sitk.GetArrayFromImage(image[:,:,i])\n",
    "        input_image = input_image.astype(np.float32)\n",
    "        input_image = ((input_image - input_image.min()) / (input_image.max() - input_image.min()))\n",
    "\n",
    "        input_image = A.Compose([\n",
    "            A.Resize(128, 128),\n",
    "            ToTensorV2()\n",
    "        ])(image=input_image)['image'].unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        ground_true_image = sitk.GetArrayFromImage(ground_true[:,:,i])\n",
    "        ground_true_image = A.Compose([\n",
    "            A.Resize(128, 128),\n",
    "            ToTensorV2()\n",
    "        ])(image=ground_true_image)['image'].unsqueeze(0).to(DEVICE)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            output_image = model(input_image)\n",
    "            output_image = torch.argmax(output_image, dim=1) \n",
    "\n",
    "        accs.append((output_image == ground_true_image).float().mean().item())\n",
    "\n",
    "\n",
    "        #Преобразование тензоров в формат, подходящий для отображения\n",
    "        input_image = input_image.squeeze().cpu().numpy()\n",
    "        output_image = output_image.squeeze().cpu().numpy()\n",
    "        ground_true_image = ground_true_image.squeeze().cpu().numpy()\n",
    "        \n",
    "        #Отображение изображений\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        ax1.imshow(input_image, cmap = 'gray')\n",
    "        ax1.set_title(f'Входное изображение. Срез {i}')\n",
    "        \n",
    "        ax2.imshow(output_image, cmap='plasma')\n",
    "        ax2.set_title(f'Сегментация нейронной сети')\n",
    "\n",
    "        ax3.imshow(ground_true_image, cmap='plasma')\n",
    "        ax3.set_title(f'Истинная маска')\n",
    "\n",
    "        accs.append((output_image == ground_true_image).mean())\n",
    "         \n",
    "        clear_output(wait = True)\n",
    "        plt.show()\n",
    "\n",
    "model = MiniUnet().to(DEVICE)\n",
    "checkpoint = torch.load('mini_unet_8.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "show_input_output(model, '1.3.6.1.4.1.14519.5.2.1.6279.6001.340012777775661021262977442176.mhd') \n",
    "print(sum(accs)/len(accs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
